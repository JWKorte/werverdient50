{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9228f8-3f2b-4c1f-9487-d76a02ad8754",
   "metadata": {},
   "source": [
    "# Version 2 mit ausschließlich kategorialen Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901f2a7-3a6a-4e4e-9c01-ef81dda7bd8b",
   "metadata": {},
   "source": [
    "Import der benutzen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3bacd-278e-46e1-9204-36bb2c8e72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbeitsbibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# Visualisierungsbibliotheken\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Preprocessing/Evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "# Mashine Learnung Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Ensemble Learning Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Feature Importance\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39c6ed-33f8-4f69-951e-14fd403a34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der im Projekt_Zensus gespeicherten Datensätze\n",
    "censusdatatrain = pd.read_csv(\"adult.data\", index_col=0)\n",
    "censusdatatest = pd.read_csv(\"adult.test\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ed1fb-ca7c-4f6a-9206-504d46e3b5ce",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9361367-b15d-4223-8729-9c9b41a108e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test-Split\n",
    "# Da der Datensatz bereits einen vorgefertigten Split mitliefert, wird auf Train_Test_Split verzichtet\n",
    "# Trainingsdaten\n",
    "X_train = censusdatatrain.drop(\"target\", axis=1) \n",
    "y_train = censusdatatrain[\"target\"].replace(\" >50K\",1).replace(\" <=50K\",0)\n",
    "# Testdaten\n",
    "X_test = censusdatatest.drop(\"target\", axis=1)\n",
    "y_test = censusdatatest[\"target\"].replace(\" >50K.\",1).replace(\" <=50K.\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfeb3b-636d-4e3d-b711-3a7e5a445c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing der Training- und Test-Daten\n",
    "for df in [X_train,X_test]:\n",
    "# workclass\n",
    "    df[\"workclass\"] = df[\"workclass\"].replace(to_replace=[\" Self-emp-not-inc\", \" Self-emp-inc\"], value=\"Selfemp\")\n",
    "    df[\"workclass\"] = df[\"workclass\"].replace(to_replace=[\" Local-gov\", \" State-gov\", \" Federal-gov\"], value=\"Goverm\")\n",
    "    df[\"workclass\"] = df[\"workclass\"].replace(to_replace=[\" ?\", \" Without-pay\", \" Never-worked\"], value=\"Residualwc\")\n",
    "# Education\n",
    "    df[\"education\"] = df[\"education\"].replace(to_replace=[\" Masters\",\" Doctorate\"], value=\"higher_academic\")\n",
    "    df[\"education\"] = df[\"education\"].replace(to_replace=[\" Assoc-voc\", \" Assoc-acdm\",\" Prof-school\"], value=\"Other\")\n",
    "    df[\"education\"] = df[\"education\"].replace(to_replace=[\" Preschool\", \" 1st-4th\", \" 5th-6th\",\" 7th-8th\", \" 9th\", \" 10th\", \" 11th\", \" 12th\"], value=\"Dropout\")\n",
    "# native country\n",
    "    df[\"native-country\"] = [1 if x==\" United-States\" or x==\" Outlying-US(Guam-USVI-etc)\" else 0 for x in df[\"native-country\"]]\n",
    "# marital-status\n",
    "    df[\"marital-status\"] = df[\"marital-status\"].replace(to_replace=[\" Married-civ-spouse\",\" Married-spouse-absent\",\" Married-AF-spouse\"], value=\"married\")\n",
    "    df[\"marital-status\"] = df[\"marital-status\"].replace(to_replace=[\" Divorced\",\" Seperated\",\" Widowed\"], value=\"seperated\")\n",
    "# race\n",
    "    df[\"race\"] = df[\"race\"].replace(to_replace=[\" Other\",\" Amer-Indian-Eskimo\",\" Asian-Pac-Islander\"], value=\"other_race\")\n",
    "# occupation\n",
    "    df[\"occupation\"] = df[\"occupation\"].replace(to_replace=[\" Armed-Forces\", \" Protective-serv\"], value=\"Security\")\n",
    "    df[\"occupation\"] = df[\"occupation\"].replace(to_replace=[\" Other-service\", \" Priv-house-serv\", \" ?\"], value=\"Other_Services\")\n",
    "# sex\n",
    "    df[\"sex\"] = [0 if x == \" Male\" else 1 for x in df[\"sex\"]]\n",
    "# capital-gain\n",
    "    df[\"oacgain\"] = [1 if x>=df[\"capital-gain\"][df[\"capital-gain\"]>0].median() else 0 for x in df[\"capital-gain\"]]\n",
    "    df[\"uacgain\"] = [1 if x<df[\"capital-gain\"][df[\"capital-gain\"]>0].median() and x>0 else 0 for x in df[\"capital-gain\"]]\n",
    "# capital-loss\n",
    "    df[\"oacloss\"] = [1 if x>=df[\"capital-loss\"][df[\"capital-loss\"]>0].median() else 0 for x in df[\"capital-loss\"]]\n",
    "    df[\"uacloss\"] = [1 if x<df[\"capital-loss\"][df[\"capital-loss\"]>0].median() and x>0 else 0 for x in df[\"capital-loss\"]]\n",
    "# age\n",
    "    df[\"oaage\"] = [1 if x>=df[\"age\"].median() else 0 for x in df[\"age\"]]\n",
    "# hours per week\n",
    "    df[\"oahpw\"] = [1 if x>=df[\"hours-per-week\"][df[\"hours-per-week\"]>0].median() else 0 for x in df[\"hours-per-week\"]]\n",
    "# Drop \n",
    "    df.drop([\"fnlwgt\", \"education-num\", \"relationship\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"age\"], axis=1, inplace=True)\n",
    "# Dummy-Variablen\n",
    "X_train = pd.concat([X_train.drop(\"workclass\", axis=1),pd.get_dummies(X_train[\"workclass\"])], axis=1)\n",
    "X_train = pd.concat([X_train.drop(\"education\", axis=1),pd.get_dummies(X_train[\"education\"])], axis=1)\n",
    "X_train = pd.concat([X_train.drop(\"occupation\", axis=1),pd.get_dummies(X_train[\"occupation\"])], axis=1)\n",
    "X_train = pd.concat([X_train.drop(\"marital-status\", axis=1),pd.get_dummies(X_train[\"marital-status\"])], axis=1)\n",
    "X_train = pd.concat([X_train.drop(\"race\", axis=1),pd.get_dummies(X_train[\"race\"])], axis=1)\n",
    "X_test = pd.concat([X_test.drop(\"marital-status\", axis=1),pd.get_dummies(X_test[\"marital-status\"])], axis=1)\n",
    "X_test = pd.concat([X_test.drop(\"education\", axis=1),pd.get_dummies(X_test[\"education\"])], axis=1)\n",
    "X_test = pd.concat([X_test.drop(\"workclass\", axis=1),pd.get_dummies(X_test[\"workclass\"])], axis=1)\n",
    "X_test = pd.concat([X_test.drop(\"occupation\", axis=1),pd.get_dummies(X_test[\"occupation\"])], axis=1)  \n",
    "X_test = pd.concat([X_test.drop(\"race\", axis=1),pd.get_dummies(X_test[\"race\"])], axis=1)\n",
    "# Drop Dummy-Variablen\n",
    "for df in [X_train,X_test]:\n",
    "    df.drop([\"Residualwc\", \"Other_Services\", \" Never-married\", \" White\", \"Dropout\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a386266-b112-4817-b9d7-446d6292f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap und Korrelationsmatrix zur Überprüfung von Multikollinearität\n",
    "for x in [X_train, X_test]:\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    display(x.corr())\n",
    "for x in [X_train, X_test]:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(x.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab17975-7ba9-41ef-8009-f7df80b04069",
   "metadata": {},
   "source": [
    "Maschine Learning Algorithmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de38b740-42cb-4908-aa70-c6fb24525525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelbuilding Mashine-Learning Classifier\n",
    "# Decision Tree\n",
    "print(\"Decision Tree\")\n",
    "start_time = time.time()\n",
    "dtc = DecisionTreeClassifier() \n",
    "tree_para = {'criterion':['gini','entropy'],'max_depth':[i for i in range(2,23)], 'min_samples_split':[i for i in range (2,23)]}\n",
    "grd_clf = HalvingGridSearchCV(dtc, tree_para, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "grd_clf.fit(X_train, y_train)\n",
    "model_with_best_tree_parameters = grd_clf.best_estimator_\n",
    "btmodel = model_with_best_tree_parameters.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(btmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(\"Modellparameter:\", btmodel.get_params())\n",
    "y_predict = btmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Gaussian Naive Bayes \n",
    "print(\"Gaussian Naive Bayes\")\n",
    "start_time = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnbmodel = gnb.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(gnbmodel,X_train,y_train,scoring=\"accuracy\",cv=5, n_jobs=-1).mean():.4f}')\n",
    "print(\"Modellparameter:\", gnbmodel.get_params())\n",
    "y_predict = gnbmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Support Vector Mashine\n",
    "print(\"Support Vector Mashine\")\n",
    "start_time = time.time()\n",
    "svm = SVC()\n",
    "svm_para = {'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "grd_clf = HalvingGridSearchCV(svm, svm_para, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "grd_clf.fit(X_train, y_train)\n",
    "model_with_best_svm_parameters = grd_clf.best_estimator_\n",
    "bsvmmodel = model_with_best_svm_parameters.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(bsvmmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(\"Modellparameter:\", bsvmmodel.get_params())\n",
    "y_predict = bsvmmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# KNeighbor Classifier\n",
    "print(\"KNeighbor Classifier\")\n",
    "start_time = time.time()\n",
    "knc = KNeighborsClassifier()\n",
    "k_para = {\"weights\" : ['uniform', 'distance'], \"leaf_size\" : [i for i in np.random.randint(2,80,15)], \"n_neighbors\": [i for i in np.random.randint(2,40,5)]}\n",
    "k_grd_clf = HalvingGridSearchCV(knc, k_para, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "k_grd_clf.fit(X_train,y_train)\n",
    "model_with_best_k_parameters = k_grd_clf.best_estimator_\n",
    "bkmodel = model_with_best_k_parameters.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(bkmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(\"Modellparameter:\", bkmodel.get_params())  \n",
    "y_predict = bkmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Ridge Classifier\n",
    "print(\"Ridge Classifier\")\n",
    "start_time = time.time()\n",
    "rc = RidgeClassifier()\n",
    "rcmodel=rc.fit(X_train,y_train)\n",
    "y_predict = rcmodel.predict(X_test)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(rcmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(rcmodel.get_params())  \n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Logistic Regression Classifier\n",
    "print(\"Logistic Regression Classifier\")\n",
    "start_time = time.time()\n",
    "lr = LogisticRegressionCV(cv=5)\n",
    "lrmodel = lr.fit(X_train,y_train)\n",
    "y_predict = lrmodel.predict(X_test)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(lrmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(\"Modellparameter:\", lrmodel.get_params())  \n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Linear Discriminant Classifier\n",
    "print(\"Linear Discriminant Classifier\")\n",
    "start_time = time.time()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "ldamodel = lda.fit(X_train,y_train)\n",
    "y_predict = ldamodel.predict(X_test)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(ldamodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(\"Modellparameter:\", ldamodel.get_params())  \n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# MLP Classifier\n",
    "print(\"MLP Classifier\")\n",
    "start_time = time.time()\n",
    "mlp = MLPClassifier()\n",
    "mlp_para = {\"max_iter\" : [i for i in [300,400]], \"learning_rate_init\": [i for i in [0.01,0.001,0.0001]], \"hidden_layer_sizes\": [i for i in [(100,),(50,),(200,),(50,25),(100,50,25),(50,100,50,25)]]}\n",
    "mlp_grd_clf = HalvingGridSearchCV(mlp, mlp_para, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "mlp_grd_clf.fit(X_train,y_train)\n",
    "model_with_best_mlp_parameters = mlp_grd_clf.best_estimator_\n",
    "bmlpmodel = model_with_best_mlp_parameters.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(bmlpmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(bmlpmodel.get_params())   \n",
    "y_predict = bmlpmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f1c67-773f-4ebb-9a0c-0cfefe512fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc-Graph Vergleich\n",
    "ax = plt.gca()\n",
    "plot_roc_curve(btmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(gnbmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(bsvmmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(bkmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(rcmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(lrmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(ldamodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(bmlpmodel,X_test,y_test,ax=ax)\n",
    "plt.savefig(\"roc_model2.png\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29647f5-4471-4c7c-ac6a-ac365c11d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance Training\n",
    "model_list = [btmodel,gnbmodel,bsvmmodel,bkmodel,rcmodel,lrmodel,ldamodel,bmlpmodel]\n",
    "file_list=[\"bt2.png\",\"gnb2.png\",\"bsv2.png\",\"bk2.png\",\"rc2.png\",\"lr2.png\",\"lda2.png\",\"bml2.png\"]\n",
    "for model, file in zip(model_list,file_list):\n",
    "    start_time = time.time()\n",
    "    result = permutation_importance(model, X_train, y_train, n_jobs=-1)\n",
    "    model_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "    model_importances.plot.bar(yerr=result.importances_std)\n",
    "    print(model)\n",
    "    print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "    plt.savefig(file)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef5f7a-130a-4c29-8205-ff7d26c3f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance Test\n",
    "model_list = [btmodel,gnbmodel,bsvmmodel,bkmodel,rcmodel,lrmodel,ldamodel,bmlpmodel]\n",
    "file_list=[\"bt2b.png\",\"gnb2b.png\",\"bsv2b.png\",\"bk2b.png\",\"rc2b.png\",\"lr2b.png\",\"lda2b.png\",\"bml2b.png\"]\n",
    "for model, file in zip(model_list,file_list):\n",
    "    start_time = time.time()\n",
    "    result = permutation_importance(model, X_test, y_test, n_jobs=-1)\n",
    "    model_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "    model_importances.plot.bar(yerr=result.importances_std)\n",
    "    print(model)\n",
    "    print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "    plt.savefig(file)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ae313-ddb2-481d-9375-c3ffc1d97876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning\n",
    "# Random Forest\n",
    "start_time = time.time()\n",
    "print(\"Random Forest\")\n",
    "rfc = RandomForestClassifier()\n",
    "tree_para = {'criterion':['gini','entropy'],'n_estimators': [i for i in [100,200,500,1000]]}\n",
    "grd_clf = HalvingGridSearchCV(rfc, tree_para, cv=5, n_jobs=-1)\n",
    "grd_clf.fit(X_train, y_train)\n",
    "model_with_best_rfc_parameters = grd_clf.best_estimator_\n",
    "brfcmodel = model_with_best_rfc_parameters.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(brfcmodel,X_train,y_train,scoring=\"accuracy\",cv=10, n_jobs=-1).mean():.4f}')\n",
    "print(brfcmodel.get_params())\n",
    "y_predict = brfcmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Gradient Boosting\n",
    "start_time = time.time()\n",
    "print(\"Gradient Boosting\")\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_para = {'n_estimators': [i for i in [50,100,500,1000]],'learning_rate': [i for i in [0.01,0.1,0.2]]}\n",
    "grd_clf = HalvingGridSearchCV(gbc, gbc_para, scoring=\"f1\", cv=5, n_jobs=-1)\n",
    "grd_clf.fit(X_train, y_train)\n",
    "model_with_best_gbc_parameters = grd_clf.best_estimator_\n",
    "bgbcmodel = model_with_best_gbc_parameters.fit(X_train,y_train)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(bgbcmodel,X_train,y_train,scoring=\"accuracy\",cv=10,n_jobs=-1).mean():.4f}')\n",
    "print(bgbcmodel.get_params())\n",
    "y_predict = bgbcmodel.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "# Bagging\n",
    "start_time = time.time()\n",
    "print(\"Bagging Classifier\")\n",
    "bc = BaggingClassifier()\n",
    "bcmodel = bc.fit(X_train,y_train)\n",
    "y_predict = bcmodel.predict(X_test)\n",
    "print(f'{\"Akkuranz: \" }{cross_val_score(bcmodel,X_train,y_train,scoring=\"accuracy\",cv=10,n_jobs=-1).mean():.4f}')\n",
    "print(bcmodel.get_params())  \n",
    "print(classification_report(y_test,y_predict))\n",
    "print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b3162-6310-4756-85f5-98c41143c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc-Graph Vergleich Ensemble\n",
    "ax = plt.gca()\n",
    "plot_roc_curve(brfcmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(bgbcmodel,X_test,y_test,ax=ax)\n",
    "plot_roc_curve(bcmodel,X_test,y_test,ax=ax)\n",
    "plt.savefig(\"roc_emodel2.png\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e0c64-f909-4adf-b3fb-07c97ae47bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance Ensemble Training\n",
    "emodel_list=[brfcmodel,bgbcmodel,bcmodel]\n",
    "file_list=[\"brf2.png\",\"bgb2.png\",\"bc2.png\"]\n",
    "for model, file in zip(emodel_list, file_list):\n",
    "    start_time = time.time()\n",
    "    result = permutation_importance(model, X_train, y_train, n_jobs=-1)\n",
    "    model_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "    model_importances.plot.bar(yerr=result.importances_std)\n",
    "    print(model)\n",
    "    print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "    plt.savefig(file)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922e182-b4ca-4e36-ad37-f5e006745dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importance Ensemble Test\n",
    "emodel_list=[brfcmodel,bgbcmodel,bcmodel]\n",
    "file_list=[\"brf2b.png\",\"bgb2b.png\",\"bc2b.png\"]\n",
    "for model, file in zip(emodel_list, file_list):\n",
    "    start_time = time.time()\n",
    "    result = permutation_importance(model, X_test, y_test, n_jobs=-1)\n",
    "    model_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "    model_importances.plot.bar(yerr=result.importances_std)\n",
    "    print(model)\n",
    "    print(f'{\"Berechnungszeit: \"}{time.time()-start_time:.2f}{\" Sekunden\"}')\n",
    "    plt.savefig(file)\n",
    "    plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
